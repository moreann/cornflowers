import pandas
import numpy as np
from sklearn.metrics import roc_auc_score

# Загрузите данные из файла data-logistic.csv. Это двумерная выборка,
# целевая переменная на которой принимает значения -1 или 1.

data = pandas.read_csv('data-logistic.csv', names=['type', 'p1', 'p2'])

yc = data['type']
X = data[['p1', 'p2']]
X1 = data['p1']
X2 = data['p2']


# евклидово расстояние между векторами весов на соседних итерациях
def distance(a, b):
    return np.sqrt(np.square(a[0]-b[0])+np.square(a[1]-b[1]))


# Обратите внимание, что мы используем полноценный градиентный спуск, а не его стохастический вариант!
def gradient(w1, w2, w1new, w2new, k, c):
    """
    Градиентный спуск для обычной и L2-регуляризованной (с коэффициентом регуляризации 10) логистической регрессии.
    Используйте длину шага k=0.1. В качестве начального приближения используйте вектор (0, 0).
    Запустите градиентный спуск и доведите до сходимости (евклидово расстояние между векторами весов на соседних
    итерациях должно быть не больше 1e-5). Рекомендуется ограничить сверху число итераций десятью тысячами.
    :param w1:  начальные веса
    :param w2:  начальные веса
    :param w1new:  начальные веса
    :param w2new:  начальные веса
    :param k: длина шага
    :param c: регуляризатора логистической регрессии
    :return: результат градиентного спуска
    """
    # вычисление весов
    for i in range(10000):
        w1new = w1 + k * np.mean(yc * X1 * (1 - (1./(1+np.exp(-yc*(w1 * X1 + w2 * X2)))))) - k*c*w1
        w2new = w2 + k * np.mean(yc * X2 * (1 - (1./(1+np.exp(-yc*(w1 * X1 + w2 * X2)))))) - k*c*w2
        if distance((w1new, w2new), (w1, w2)) < 0.00001:
            break
        w1, w2 = w1new, w2new

    # вычисление градиентного спуска
    result = []
    for i in range(len(X)):
        pred = 1/(1+np.exp(-w1 * X1[i] - w2 * X2[i]))
        result.append(pred)
    return result


# Какое значение принимает AUC-ROC на обучении без регуляризации и при ее использовании?
# Эти величины будут ответом на задание.
# В качестве ответа приведите два числа через пробел.
# Обратите внимание, что на вход функции roc_auc_score нужно подавать оценки вероятностей,
# подсчитанные обученным алгоритмом. Для этого воспользуйтесь сигмоидной функцией: a(x) = 1 / (1 + exp(-w1 x1 - w2 x2)).
y = np.array(data['type'])
r1 = gradient(w1=0, w2=0, w1new=0, w2new=0, k=0.1, c=0)
r2 = gradient(w1=0, w2=0, w1new=0, w2new=0, k=0.1, c=10)
a = roc_auc_score(yc, r1)
b = roc_auc_score(yc, r2)
print(round(a, 3))
print(round(b, 3))
# 0.927
# 0.936

# Попробуйте поменять длину шага. Будет ли сходиться алгоритм, если делать более длинные шаги?
# Как меняется число итераций при уменьшении длины шага?
r3 = gradient(w1=0, w2=0, w1new=0, w2new=0, k=0.5, c=0)
r4 = gradient(w1=0, w2=0, w1new=0, w2new=0, k=0.1, c=10)
r = roc_auc_score(y, r3)
n = roc_auc_score(y, r4)
print(round(r, 3))
print(round(n, 3))
# При изменении шагов до опредленного значения не регуляризованная функция не изменяется. с регуляризованной тоже самое

# Попробуйте менять начальное приближение. Влияет ли оно на что-нибудь?
r5 = gradient(w1=10, w2=-7, w1new=10, w2new=-7, k=0.1, c=0)
r6 = gradient(w1=10, w2=-7, w1new=10, w2new=-7, k=0.1, c=10)
t = roc_auc_score(y, r5)
p = roc_auc_score(y, r6)
print(round(t, 3))
print(round(p, 3))
# не влияет
